1. Tensor Initialization for Weights and Bias
Weights Initialization:
-Use Xavier initialization by scaling with 1/sqrt(n)
    weights = torch.randn(784, 10) / math.sqrt(784)
    weights.requires_grad_()

Bias Initialization:
-Initialized to zeros with gradient tracking enabled.
    bias = torch.zeros(10, requires_grad=True)

Gradient Tracking:
-Setting requires_grad=True enables PyTorch to automatically compute gradients for backpropagation.

2. Defining the Model and Activation Function
Linear Model:
-Uses matrix multiplication (@) and broadcasted addition to combine
  inputs with weights and bias.
-example
    def model(xb):
    return log_softmax(xb @ weights + bias)

Log-Softmax Activation Function:
A custom implementation of log-softmax:
    def log_softmax(x):
    return x - x.exp().sum(-1).log().unsqueeze(-1)

3. Making Predictions (Forward Pass)
-Batch Processing:
--Select a mini-batch of data (e.g., 64 samples)
    bs = 64
    xb = x_train[0:bs]
    preds = model(xb)
    print(preds[0], preds.shape)
-Output:
--preds tensor contains values and a gradient function (grad_fn) for     backpropagation.

4. Loss Function and Accuracy Calculation
Negative Log-Likelihood (NLL) Loss:
    def nll(input, target):
        return -input[range(target.shape[0]), target].mean()
    loss_func = nll
Accuracy Calculation:
    def accuracy(out, yb):
        preds = torch.argmax(out, dim=1)
        return (preds == yb).float().mean()
Example Usage:
    print(loss_func(preds, y_train[0:bs]))
    print(accuracy(preds, y_train[0:bs]))
    
5. Training Loop (Backpropagation)
Steps per Epoch:

Select a mini-batch of data.
Use the model to generate predictions.
Calculate the loss.
Perform backpropagation using loss.backward().
Weight and Bias Update:

Use torch.no_grad() to prevent recording operations during gradient updates:

    with torch.no_grad():
        weights -= weights.grad * lr
        bias -= bias.grad * lr
        weights.grad.zero_()
        bias.grad.zero_()
Training Code Example:
    lr = 0.5  # learning rate
    epochs = 2  # number of training iterations

    for epoch in range(epochs):
        for i in range((n - 1) // bs + 1):
            start_i = i * bs
            end_i = start_i + bs
            xb = x_train[start_i:end_i]
            yb = y_train[start_i:end_i]
            pred = model(xb)
            loss = loss_func(pred, yb)

            loss.backward()
            with torch.no_grad():
                weights -= weights.grad * lr
                bias -= bias.grad * lr
                weights.grad.zero_()
                bias.grad.zero_()

6. Debugging with Python Debugger
You can use the Python debugger to step through the training process by uncommenting the following line:

    from IPython.core.debugger import set_trace
    # set_trace()

7. Final Model Evaluation
After training, the loss should decrease and accuracy should improve compared to the initial values.



