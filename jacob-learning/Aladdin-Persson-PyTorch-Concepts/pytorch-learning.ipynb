{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook on AP's video tutorial\n",
    "\n",
    "This is the first time im choosing to use a jupyter notebook, but I think its going to go really well for this style of coding. I'm excited actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing torch to use the PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "built some tensors, the main datastructure used in machine learning. The tensor datastructure has a few cool arguments\n",
    "- first one is the matrix\n",
    "- second one is the datatype of the elements\n",
    "- third one specifies a device. This can be the GPU (cuda) or the CPU (default)\n",
    "- fourth one specifies if this tensor requires an *autograd*, pytorch's backpropagation algorithm\n",
    "\n",
    "by convention, you specify the device as \n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "```\n",
    "\n",
    "as this automatically handles the decision as the resource is available on a user's machine.\n",
    "\n",
    "tensor objects have a stylized print method too, which is cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "int_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "float_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "cuda_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=my_device)\n",
    "backprop_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=my_device, requires_grad=True)\n",
    "\n",
    "print(int_tensor)\n",
    "print(float_tensor)\n",
    "print(\"\\n\")\n",
    "print(\"backprop tensor:  \", backprop_tensor)\n",
    "print(\"backprop tensor device:  \",backprop_tensor.device)\n",
    "print(\"backprop tensor shape:  \",backprop_tensor.shape)\n",
    "print(\"backprop tensor datatype:  \",backprop_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other ways you can initialize a tensor\n",
    "- ```torch.empty()``` uses garbage memory. dont use this unless you know what you're doing\n",
    "- ```torch.zeros()``` initializes all elements of the tensor\n",
    "- ``torch.rand()`` initializes elements with values picked from a normal distribution\n",
    "- `torch.ones()` is pretty obvious, like zeroes\n",
    "- ```torch.eye()``` creates an identity matrix. note its size arguments are not inside a tuple\n",
    "- build a range of values with ```torch.arange()``` from start to end, by incrementing step value\n",
    "- build another range with `torch.linspace()` from start to end, but steps determines the number of entries and evenly increments all elements\n",
    "\n",
    "- `torch.diag(torch.some-tensor)` preserves values along the diagonal of the tensor argument, like multiplying with an identity matrix\n",
    "\n",
    "- `torch.uniform()` uses a uniform dist instead of norm\n",
    "\n",
    "you can also chain these initialization statements, like building the tensor with garbage memory and a fixed size, then calling .rand() on it. note that when calling the second function, convention has you place an underscore before the \"()\"\n",
    "\n",
    "```python\n",
    "x = torch.empty(size=(1,5)).normal_(mean=0, std=1)\n",
    "```\n",
    "note the underscore after normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(size = (3, 3))\n",
    "y = torch.zeros((3, 3)) #size not needed to be specified\n",
    "z = torch.rand((3, 3))\n",
    "ones = torch.ones((3, 3))\n",
    "i = torch.eye(5, 5)\n",
    " \n",
    "sequence = torch.arange(start=0, end=5, step=1)\n",
    "other_sequence = torch.linspace(start=0.1, end=1, steps=10)\n",
    "\n",
    "\n",
    "''' \n",
    "\n",
    "print(\"empty array is composed of garbage memory:\\n\", x)\n",
    "print(\"zeroes array is initialized zeroes:\\n\", y)\n",
    "print(\"random array picks values from a norm dist:\\n\", z)\n",
    "print(\"ones is pretty obvious:\\n\", ones)\n",
    "print(\"i is the identity matrix. used in lin alg:\\n\", i)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "print('\\nsequences\\n')\n",
    "print(\"sequence A:\\n\", sequence)\n",
    "print(\"sequence B:\\n\", other_sequence)\n",
    "\n",
    "print(\"diagonal:\\n\", torch.diag(torch.rand((3, 3))))\n",
    "print(\"chaining:\\n\", torch.empty(size=(3,3)).uniform_(0,1))\n",
    "print(\"more chaining:\\n\", torch.empty(size=(1,5)).normal_(mean=0, std=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a tensor to another datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(4) # [0, 1, 2, 3]\n",
    "\n",
    "print(tensor.bool())\n",
    "print(tensor.short()) #int16\n",
    "print(tensor.long()) #int64\n",
    "print(tensor.half())\n",
    "print(tensor.float())\n",
    "print(tensor.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert numpy arrays to tensors like this. note, there might be rounding errors when performing this back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_array = np.zeros((5,5))\n",
    "tensor = torch.from_numpy(np_array)\n",
    "\n",
    "np_array_original = tensor.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor math\n",
    "\n",
    "Now that we know how tensors work alone, lets see how they do together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([9, 8 ,7])\n",
    "\n",
    "# Addition\n",
    "z1 = torch.empty(3) # empty tensor of 3 entries\n",
    "torch.add(x, y, out=z1)\n",
    "print(z1)\n",
    "\n",
    "z2 = torch.add(x, y)\n",
    "z = x + y\n",
    "\n",
    "print(\"all three are the same\")\n",
    "print(\"z1:\\n\",z1)\n",
    "print(\"z2:\\n\",z2)\n",
    "print(\"z:\\n\",z)\n",
    "\n",
    "z = x - y\n",
    "print(\"subtraction:\\n\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([9, 8 ,7])\n",
    "\n",
    "#division\n",
    "z = torch.true_divide(x, y)\n",
    "print(\"division:\\n\",z)\n",
    "\n",
    "#+=\n",
    "t1 = torch.zeros(3)\n",
    "t2 = torch.zeros(3)\n",
    "t1.add_(x)\n",
    "t2 += x\n",
    "print(\"+= operator:\\n\", t1, \"\\n\", t2)\n",
    "\n",
    "#exponentiation\n",
    "z1 = x.pow(2)\n",
    "z2 = x ** 2\n",
    "print(\"exponentiation:\\n\", z1, \"\\n\", z2)\n",
    "\n",
    "#inequalities\n",
    "z1 = x > 0\n",
    "z2 = x < 0\n",
    "\n",
    "print(\"x > 0:\\n\", z1)\n",
    "print(\"x < 0:\\n\", z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix operations are important in machine learning. lets go over a few important ones in pytorch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
